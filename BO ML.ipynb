{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging, timeit\n",
    "#from btEngine2.DataLoader import DataLoader\n",
    "from btEngine2.MarketData import MarketData\n",
    "from btEngine2.TradingRule import TradingRule\n",
    "\n",
    "\n",
    "import platform\n",
    "import pandas as pd\n",
    "\n",
    "pd.options.display.float_format = lambda x: f'{x:,.0f}' if abs(x) >= 1000 else (f'{x:.2f}' if abs(x) < 10 else f'{x:.1f}')\n",
    "# Detect operating system\n",
    "if platform.system() == \"Windows\":\n",
    "    ticker_csv_path = r'G:\\Projects\\BackTesting1.0\\Data\\Inputs\\TickerList-Futs.csv'\n",
    "    save_directory = r\"G:\\Projects\\BackTesting1.0\\Data\\Bloomberg\\Futures\"\n",
    "    helper_directory = r'G:\\Projects\\BackTesting1.0\\Data\\Bloomberg\\HelperFiles'\n",
    "    bt_folder = r'BackTests\\bo_resrch_ml'\n",
    "    av_folder = r'G:\\Projects\\BackTesting1.0\\Data\\Inputs\\AssetSizing-Futs.csv'\n",
    "else:  # Assume macOS for other cases\n",
    "    ticker_csv_path = r'Data/Inputs/TickerList-Futs.csv'\n",
    "    save_directory = r\"Data/Bloomberg/Futures\"\n",
    "    helper_directory = r'Data/Bloomberg/HelperFiles'\n",
    "    bt_folder = r'BackTests/bo_resrch_ml'\n",
    "    av_folder = r'Data/Inputs/AssetSizing-Futs.csv'\n",
    "\n",
    "\n",
    "\n",
    "# Define paths to auxiliary data for MarketData\n",
    "tick_values_path = os.path.join(helper_directory, 'fut_val_pt.parquet')\n",
    "fx_rates_path = os.path.join(helper_directory, 'fxHist.parquet')\n",
    "\n",
    "# Initialize the MarketData\n",
    "market_data = MarketData(\n",
    "    base_directory=save_directory,\n",
    "    tick_values_path=tick_values_path,\n",
    "    fx_rates_path=fx_rates_path,\n",
    "    instrument_type=\"Futures\",\n",
    "    n_threads=8,  # Number of threads for parallel data loading\n",
    "    log_level=logging.ERROR  # Set to DEBUG for more detailed logs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (2_218, 14)\n",
      "┌────────────┬───────┬───────┬───────┬───┬─────────┬─────────┬─────────────────┬────────────────┐\n",
      "│ Date       ┆ Open  ┆ High  ┆ Low   ┆ … ┆ BadOHLC ┆ FX_Rate ┆ Tick_Value_Base ┆ Tick_Value_USD │\n",
      "│ ---        ┆ ---   ┆ ---   ┆ ---   ┆   ┆ ---     ┆ ---     ┆ ---             ┆ ---            │\n",
      "│ date       ┆ f64   ┆ f64   ┆ f64   ┆   ┆ bool    ┆ f64     ┆ f64             ┆ f64            │\n",
      "╞════════════╪═══════╪═══════╪═══════╪═══╪═════════╪═════════╪═════════════════╪════════════════╡\n",
      "│ 2016-01-04 ┆ 49.95 ┆ 49.95 ┆ 49.8  ┆ … ┆ false   ┆ 1.0     ┆ 250.0           ┆ 250.0          │\n",
      "│ 2016-01-05 ┆ 50.5  ┆ 50.5  ┆ 50.5  ┆ … ┆ true    ┆ 1.0     ┆ 250.0           ┆ 250.0          │\n",
      "│ 2016-01-06 ┆ 50.15 ┆ 50.15 ┆ 50.15 ┆ … ┆ false   ┆ 1.0     ┆ 250.0           ┆ 250.0          │\n",
      "│ 2016-01-07 ┆ 49.9  ┆ 49.9  ┆ 49.65 ┆ … ┆ false   ┆ 1.0     ┆ 250.0           ┆ 250.0          │\n",
      "│ 2016-01-08 ┆ 49.9  ┆ 49.9  ┆ 49.65 ┆ … ┆ false   ┆ 1.0     ┆ 250.0           ┆ 250.0          │\n",
      "│ …          ┆ …     ┆ …     ┆ …     ┆ … ┆ …       ┆ …       ┆ …               ┆ …              │\n",
      "│ 2024-10-15 ┆ 76.25 ┆ 76.4  ┆ 76.25 ┆ … ┆ false   ┆ 1.0     ┆ 250.0           ┆ 250.0          │\n",
      "│ 2024-10-16 ┆ 76.25 ┆ 76.85 ┆ 76.25 ┆ … ┆ false   ┆ 1.0     ┆ 250.0           ┆ 250.0          │\n",
      "│ 2024-10-17 ┆ 76.9  ┆ 76.95 ┆ 76.85 ┆ … ┆ false   ┆ 1.0     ┆ 250.0           ┆ 250.0          │\n",
      "│ 2024-10-18 ┆ 76.85 ┆ 76.85 ┆ 76.7  ┆ … ┆ false   ┆ 1.0     ┆ 250.0           ┆ 250.0          │\n",
      "│ 2024-10-21 ┆ 77.0  ┆ 77.0  ┆ 77.0  ┆ … ┆ true    ┆ 1.0     ┆ 250.0           ┆ 250.0          │\n",
      "└────────────┴───────┴───────┴───────┴───┴─────────┴─────────┴─────────────────┴────────────────┘\n",
      "Total tickers loaded: 93\n"
     ]
    }
   ],
   "source": [
    "tick = 'ASD3 Index'\n",
    "# Access data for a specific ticker\n",
    "try:\n",
    "    test_df = market_data.get_ticker_data(tick)\n",
    "    print(test_df)\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "\n",
    "# Access all preprocessed data\n",
    "all_data = market_data.get_data()\n",
    "print(f\"Total tickers loaded: {len(all_data)}\")\n",
    "\n",
    "# Access FX rates\n",
    "fx_rates = market_data.get_fx_rates()\n",
    "# Access tick values\n",
    "tick_values = market_data.get_tick_values()\n",
    "# Access asset classes\n",
    "asset_classes = market_data.get_asset_classes()\n",
    "\n",
    "test_df.to_pandas().to_clipboard()\n",
    "#market_data = market_data.date_filter(start_date='01012010')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from btEngine2.Rules.Momentum.sbo_long import *\n",
    "from btEngine2.Rules.Momentum.sbo_short import *\n",
    "\n",
    "\n",
    "\n",
    "rsi_l = dict(rsi_param=(7, 90), N=8)\n",
    "rsi_l_qe = dict(rsi_param=(7, 90), N=8,exit_quick_rule=True)\n",
    "rsi_l_qed = dict(rsi_param=(7, 90), N=8, N_min=2, exit_quick_rule=True)\n",
    "rsi_l_ml_l = dict(rsi_param=(7, 90), N=8, min_dps=5, max_dps=30, probability_threshold=0.5, trend_filter=(100, 'ema'),exit_quick_rule=True)\n",
    "rsi_l_ml_h = dict(rsi_param=(7, 90), N=8, min_dps=5, max_dps=30, probability_threshold=0.75, trend_filter=(100, 'ema'),exit_quick_rule=True)\n",
    "rsi_l_ml_fade_h = dict(rsi_param=(7, 90), N=4, min_dps=5, max_dps=30, probability_threshold=-0.1)\n",
    "rsi_l_ml_fade_m = dict(rsi_param=(7, 90), N=4, min_dps=5, max_dps=30, probability_threshold=-0.2)\n",
    "rsi_l_ml_fade_l = dict(rsi_param=(7, 90), N=4, min_dps=5, max_dps=30, probability_threshold=-0.4)\n",
    "\n",
    "\n",
    "pSizeParamsUse = {\n",
    "    'AssetVol': av_folder,  # Target asset volatility in USD\n",
    "    'VolLookBack': 30,\n",
    "    'VolMethod': 'ewm'  # Lookback period for volatility calculation\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_variable_name(var, globals_dict):\n",
    "    return [name for name in globals_dict if globals_dict[name] is var]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rsi_param': (7, 90), 'N': 8}\n",
      "name: rsi_l\n",
      "---------------------------------\n",
      "{'rsi_param': (7, 90), 'N': 8, 'exit_quick_rule': True}\n",
      "name: rsi_l_qe\n",
      "---------------------------------\n",
      "{'rsi_param': (7, 90), 'N': 8, 'N_min': 2, 'exit_quick_rule': True}\n",
      "name: rsi_l_qed\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rsi_l': <btEngine2.TradingRule.TradingRule at 0x3020cf110>,\n",
       " 'rsi_l_qe': <btEngine2.TradingRule.TradingRule at 0x14e5ce1b0>,\n",
       " 'rsi_l_qed': <btEngine2.TradingRule.TradingRule at 0x3020cff20>}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trading_rules_dict = {}\n",
    "\n",
    "# Define the trading rule functions and their corresponding parameters\n",
    "trading_rule_functions = {\n",
    "    'rsi_l': (bo_rsi_long, [rsi_l, rsi_l_qe, rsi_l_qed])\n",
    "    #'rsi_l_ml_h': (bo_rsi_long_ml, [rsi_l_ml_l, rsi_l_ml_h, rsi_l_ml_fade_h, rsi_l_ml_fade_m, rsi_l_ml_fade_l])\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Loop through each trading rule function and its parameters\n",
    "for rule_name, (rule_function, params_list) in trading_rule_functions.items():\n",
    "    for params in params_list:\n",
    "        print(params)\n",
    "        print(f'name: {get_variable_name(params, globals())[0]}')\n",
    "        print('---------------------------------')\n",
    "        rule_label = get_variable_name(params, globals())[0]\n",
    "        trading_rules_dict[rule_label] = TradingRule(\n",
    "            market_data=market_data,\n",
    "            trading_rule_function=rule_function,\n",
    "            trading_params=params,\n",
    "            position_sizing_params=pSizeParamsUse,\n",
    "            cont_rule=False,\n",
    "            name_label=rule_label,\n",
    "            bt_folder=bt_folder\n",
    "        )\n",
    "\n",
    "trading_rules_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_dfs = {}\n",
    "pnl_dfs = {}\n",
    "for tr in trading_rules_dict.values():\n",
    "    tr.backtest_all_assets(save=True)\n",
    "    tmpdf = tr.perf_table(byac=True,metric='sharpe')\n",
    "    perf_dfs[tr.name_lbl] = tmpdf\n",
    "    tmpdf = tr.perf_table(byac=True, metric='pnl')\n",
    "    pnl_dfs[tr.name_lbl] = tmpdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnl_dfs['rsi_l_ml_h']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize a dictionary to store the equity plots\n",
    "equity_plots = {}\n",
    "\n",
    "# Iterate over each trading rule in the rsi_trading_rules dictionary\n",
    "for rule_name, trading_rule in trading_rules_dict.items():\n",
    "    # Plot the equity for the trading rule\n",
    "    equity_plot = trading_rule.plot_equity(byac=True, excl_ac=['eq-vol', 'fx-em'])\n",
    "    equity_plots[rule_name] = equity_plot\n",
    "\n",
    "# Plot all the equity curves in a single figure\n",
    "plt.figure(figsize=(12, 8))\n",
    "for rule_name, equity_plot in equity_plots.items():\n",
    "    plt.plot(equity_plot.index, equity_plot['Total'], label=rule_name)\n",
    "\n",
    "plt.title('Equity Curves for Trading Rules')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Equity')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty DataFrame to store the collated results\n",
    "collated_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over the equity_plots dictionary\n",
    "for key, df in equity_plots.items():\n",
    "    # Select the 'Total' column and rename it to the key\n",
    "    total_col = df[['Total']].rename(columns={'Total': key})\n",
    "    # Concatenate the renamed column to the collated_df\n",
    "    collated_df = pd.concat([collated_df, total_col], axis=1)\n",
    "\n",
    "collated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnls_df = collated_df.diff()\n",
    "\n",
    "n = 125\n",
    "vol_target = 10000000\n",
    "# Calculate the rolling annualized volatility\n",
    "rolling_vol = pnls_df.rolling(window=n).std() * (252 ** 0.5)\n",
    "\n",
    "# Initialize a DataFrame to store the scaled PnL values\n",
    "scaled_pnls_df = pnls_df.copy()\n",
    "\n",
    "# Scale the PnL columns to match the target volatility\n",
    "for col in pnls_df.columns:\n",
    "    # Avoid division by zero or NaN issues by filtering valid volatilities\n",
    "    valid_vol = rolling_vol[col].ffill()\n",
    "    # Scale factor to match the target volatility\n",
    "    scale_factor = vol_target / valid_vol\n",
    "    # Apply scaling, keeping the existing NaNs from the rolling window calculation\n",
    "    scaled_pnls_df[col] = pnls_df[col] * scale_factor\n",
    "\n",
    "scaled_pnls_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df = scaled_pnls_df.cumsum()\n",
    "\n",
    "scaled_df.plot(title='Scaled PnLs', figsize=(16,9), grid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "scaled_ret = scaled_pnls_df\n",
    "lb = 1\n",
    "\n",
    "mean_ret = scaled_ret[-lb*252:].mean()\n",
    "std_ret = scaled_ret[-lb*252:].std()\n",
    "\n",
    "annualized_mean_ret = mean_ret * 252\n",
    "annualized_std_ret = std_ret * (252 ** 0.5)\n",
    "\n",
    "sharpes = annualized_mean_ret / annualized_std_ret\n",
    "\n",
    "sharpes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the rolling window size for 3 years (252 trading days per year)\n",
    "rolling_window = 252 * 1\n",
    "\n",
    "# Initialize a dictionary to store the rolling Sharpe ratios\n",
    "rolling_sharpe_ratios = {}\n",
    "\n",
    "\n",
    "# Calculate the rolling Sharpe ratio for each strategy\n",
    "for col in scaled_ret.columns:\n",
    "    rolling_mean = scaled_ret[col].rolling(window=rolling_window).mean()\n",
    "    rolling_std = scaled_ret[col].rolling(window=rolling_window).std()\n",
    "    annualized_mean = rolling_mean * 252\n",
    "    annualized_std = rolling_std * (252 ** 0.5)\n",
    "    rolling_sharpe = annualized_mean / annualized_std\n",
    "    rolling_sharpe_ratios[col] = rolling_sharpe\n",
    "\n",
    "# Plot the rolling Sharpe ratios\n",
    "plt.figure(figsize=(14, 10))\n",
    "for col, rolling_sharpe in rolling_sharpe_ratios.items():\n",
    "    mov_avg = rolling_sharpe.rolling(window=60).mean()\n",
    "    plt.plot(mov_avg, label=col)\n",
    "\n",
    "plt.title('Rolling 3-Year Annualized Sharpe Ratios')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sharpe Ratio')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "scaled_ret.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "bo_pf = scaled_pnls_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ensure that 'Date' is the index and in datetime format if not already\n",
    "pnl_series = bo_pf['rsi_l_ml_h']\n",
    "pnl_series.index = pd.to_datetime(pnl_series.index)\n",
    "\n",
    "# Resample to monthly frequency and sum the PnL for each month\n",
    "monthly_pnl = pnl_series.resample('M').sum()\n",
    "\n",
    "# Create a DataFrame for monthly PnL\n",
    "monthly_pnl_df = monthly_pnl.to_frame(name='Skew Long and BO L Pullback').reset_index()\n",
    "monthly_pnl_df['Year'] = monthly_pnl_df['Date'].dt.year\n",
    "monthly_pnl_df['Month'] = monthly_pnl_df['Date'].dt.month_name().str[:3]\n",
    "\n",
    "# Pivot the DataFrame to get the desired format\n",
    "monthly_pnl_pivot = monthly_pnl_df.pivot(index='Year', columns='Month', values='Skew Long and BO L Pullback')\n",
    "monthly_pnl_pivot = monthly_pnl_pivot[['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']]\n",
    "\n",
    "# Calculate the total PnL for each year\n",
    "monthly_pnl_pivot['Total'] = monthly_pnl_pivot.sum(axis=1)\n",
    "\n",
    "# Calculate annual Sharpe using daily data\n",
    "# Group daily data by year for the sum of returns and volatility\n",
    "annual_returns = pnl_series.resample('Y').sum()  # Total return per year\n",
    "annual_volatility = pnl_series.resample('Y').std() * (252 ** 0.5)  # Annualized volatility\n",
    "\n",
    "# Calculate Sharpe ratio and handle cases with zero volatility\n",
    "annual_sharpe = (annual_returns / annual_volatility).replace([float('inf'), -float('inf')], pd.NA)\n",
    "\n",
    "# Merge annual Sharpe ratio into the pivot table\n",
    "monthly_pnl_pivot['Sharpe'] = annual_sharpe.values\n",
    "\n",
    "monthly_pnl_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "readme_path = r'BackTests/skew_resrch/abs_skew_long_4a4bb9_e7b522_skew_long_252/README.txt'\n",
    "\n",
    "exclude = ['eq-divf', 'eq-vol'] + [x for x in asset_classes if 'comm-' in x] + [x for x in asset_classes if 'fx-' in x] + ['stir'] + ['eq-as'] + ['crypto']\n",
    "\n",
    "# Initialize TradingRule from README file\n",
    "skew_long = TradingRule.from_readme(\n",
    "    readme_path=readme_path,\n",
    "    market_data=market_data,\n",
    "    sizing_rf=0.25,\n",
    "    excl_ac=exclude,\n",
    "    log_level=logging.ERROR,\n",
    "    # Include any other necessary arguments\n",
    ")\n",
    "\n",
    "readme_path = r'BackTests/skew_resrch/abs_skew_short_6c32b2_e7b522_skew_long_252/README.txt'\n",
    "skew_short = TradingRule.from_readme(\n",
    "    readme_path=readme_path,\n",
    "    market_data=market_data,\n",
    "    sizing_rf=0.25,\n",
    "    excl_ac=exclude,\n",
    "    log_level=logging.ERROR,\n",
    "    # Include any other necessary arguments\n",
    ")\n",
    "\n",
    "readme_path = r'BackTests/skew_resrch/abs_skew_combined_e32ea1_e7b522_skew_ls_252/README.txt'\n",
    "skew_combined = TradingRule.from_readme(\n",
    "    readme_path=readme_path,\n",
    "    market_data=market_data,\n",
    "    sizing_rf=0.25,\n",
    "    excl_ac=exclude,\n",
    "    log_level=logging.ERROR,\n",
    "    # Include any other necessary arguments\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skew_long_pnl = skew_long.plot_equity(byac=True)\n",
    "skew_short_pnl = skew_short.plot_equity(byac=True)\n",
    "skew_comb_pnl = skew_combined.plot_equity(byac=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and rename the 'Total' columns\n",
    "skew_long_total = skew_long_pnl[['Total']].rename(columns={'Total': 'Skew_Long_252'})\n",
    "skew_short_total = skew_short_pnl[['Total']].rename(columns={'Total': 'Skew_Short_252'})\n",
    "skew_comb_total = skew_comb_pnl[['Total']].rename(columns={'Total': 'Skew_Comb_252'})\n",
    "\n",
    "# Concatenate the renamed columns to concat_df\n",
    "concat_df = pd.concat([bo_pf_cum, skew_long_total, skew_short_total, skew_comb_total], axis=1)\n",
    "\n",
    "concat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pnl = concat_df.diff().dropna()\n",
    "df_pnl.cumsum().plot(figsize=(12, 6), title='Cumulative PnL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your volatility targets\n",
    "vol_targets = {'fast_pb': 10000000, 'rsi_slow_tf': 10000000, 'Skew_Long_252': 10000000, 'Skew_Short_252': 10000000, 'Skew_Comb_252': 10000000}\n",
    "\n",
    "# Calculate the rolling 252-period annualized volatility for each column\n",
    "# Annualized vol = std of daily returns * sqrt(252)\n",
    "rolling_vol = df_pnl.rolling(window=63).std() * (252 ** 0.5)\n",
    "display(rolling_vol)\n",
    "# Create a new dataframe to store scaled PnL values\n",
    "scaled_df = df_pnl.copy()\n",
    "\n",
    "# Scale the PnL columns to match the target volatility\n",
    "for col in df_pnl.columns:\n",
    "    if col in vol_targets:\n",
    "        # Avoid division by zero or NaN issues by filtering valid volatilities\n",
    "        valid_vol = rolling_vol[col].ffill()\n",
    "        # Scale factor to match the target volatility\n",
    "        scale_factor = vol_targets[col] / valid_vol \n",
    "        # Apply scaling, keeping the existing NaNs from the rolling window calculation\n",
    "        scaled_df[col] = df_pnl[col] * scale_factor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_pnl = scaled_df.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_pnl.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the daily mean returns\n",
    "daily_mean_returns = df_pnl[-252*10:].mean()\n",
    "\n",
    "# Calculate the daily standard deviation of returns\n",
    "daily_std_returns = df_pnl[-252*5:].std()\n",
    "\n",
    "# Annualize the mean returns and standard deviation\n",
    "annualized_mean_returns = daily_mean_returns * 252\n",
    "annualized_std_returns = daily_std_returns * (252 ** 0.5)\n",
    "\n",
    "# Compute the Sharpe ratio\n",
    "sharpe_ratios = annualized_mean_returns / annualized_std_returns\n",
    "\n",
    "sharpe_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_pnl['mom_strats'] = scaled_pnl['fast_pb'] + scaled_pnl['rsi_slow_tf']\n",
    "scaled_pnl['bo fast/slow + long skew'] = scaled_pnl['mom_strats'] + scaled_pnl['Skew_Long_252']\n",
    "scaled_pnl['bo fast/slow + comb skew'] = scaled_pnl['mom_strats'] + scaled_pnl['Skew_Comb_252']\n",
    "scaled_pnl['bo slow + comb skew'] = scaled_pnl['rsi_slow_tf'] + scaled_pnl['Skew_Comb_252']\n",
    "scaled_pnl['bo slow + long skew'] = scaled_pnl['rsi_slow_tf'] + scaled_pnl['Skew_Long_252']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_pnl.plot(figsize=(16,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_ret = scaled_pnl.diff().dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = 15\n",
    "\n",
    "scaledf = scaled_ret[-lb*252:]\n",
    "\n",
    "mean_ret = scaledf.mean()\n",
    "std_ret = scaledf.std()\n",
    "\n",
    "annualized_mean_ret = mean_ret * 252\n",
    "annualized_std_ret = std_ret * (252 ** 0.5)\n",
    "\n",
    "sharpes = annualized_mean_ret / annualized_std_ret\n",
    "\n",
    "sharpes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_ret.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pnl = scaled_ret\n",
    "\n",
    "rolling_vol = df_pnl.rolling(window=252).std() * (252 ** 0.5)\n",
    "\n",
    "# Create a new dataframe to store scaled PnL values\n",
    "scaled_df = df_pnl.copy()\n",
    "\n",
    "# Scale the PnL columns to match the target volatility\n",
    "for col in df_pnl.columns:\n",
    "    if col in vol_targets:\n",
    "        # Avoid division by zero or NaN issues by filtering valid volatilities\n",
    "        valid_vol = rolling_vol[col].ffill()\n",
    "        # Scale factor to match the target volatility\n",
    "        scale_factor = 10000000 / valid_vol\n",
    "        # Apply scaling, keeping the existing NaNs from the rolling window calculation\n",
    "        scaled_df[col] = df_pnl[col] * scale_factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df = scaled_df.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df.iloc[:,-12:].plot(figsize=(16,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = 1\n",
    "scaled_ret = scaled_pnl.diff().dropna()\n",
    "\n",
    "\n",
    "scaledf = scaled_ret[-lb*252:]\n",
    "\n",
    "mean_ret = scaledf.mean()\n",
    "std_ret = scaledf.std()\n",
    "\n",
    "annualized_mean_ret = mean_ret * 252\n",
    "annualized_std_ret = std_ret * (252 ** 0.5)\n",
    "\n",
    "sharpes = annualized_mean_ret / annualized_std_ret\n",
    "\n",
    "sharpes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_drawdowns(pnl_series):\n",
    "    \"\"\"\n",
    "    Calculate the drawdowns of a PnL series.\n",
    "    \"\"\"\n",
    "    # Calculate the cumulative returns\n",
    "    cumulative = pnl_series.cumsum()\n",
    "    # Calculate the running maximum\n",
    "    running_max = cumulative.cummax()\n",
    "    # Calculate the drawdown\n",
    "    drawdown = cumulative - running_max\n",
    "    # Calculate the duration of the drawdown\n",
    "    drawdown_duration = (drawdown < 0).astype(int).groupby((drawdown >= 0).astype(int).cumsum()).cumsum()\n",
    "    \n",
    "    return drawdown, drawdown_duration\n",
    "\n",
    "# Initialize dictionaries to store results\n",
    "worst_drawdowns = {}\n",
    "average_drawdowns = {}\n",
    "average_drawdown_durations = {}\n",
    "\n",
    "# Iterate over each column in scaled_ret\n",
    "for col in scaled_ret.columns:\n",
    "    drawdown, drawdown_duration = calculate_drawdowns(scaled_ret[col])\n",
    "    \n",
    "    # Store the worst drawdown\n",
    "    worst_drawdowns[col] = drawdown.min()\n",
    "    # Store the average drawdown\n",
    "    average_drawdowns[col] = drawdown[drawdown < 0].mean()\n",
    "    # Store the average drawdown duration\n",
    "    average_drawdown_durations[col] = drawdown_duration[drawdown_duration > 0].mean()\n",
    "\n",
    "# Convert results to DataFrames for better readability\n",
    "worst_drawdowns_df = pd.DataFrame.from_dict(worst_drawdowns, orient='index', columns=['Worst Drawdown'])\n",
    "average_drawdowns_df = pd.DataFrame.from_dict(average_drawdowns, orient='index', columns=['Average Drawdown'])\n",
    "average_drawdown_durations_df = pd.DataFrame.from_dict(average_drawdown_durations, orient='index', columns=['Average Drawdown Duration'])\n",
    "\n",
    "# Display the results\n",
    "print(\"Worst Drawdowns:\")\n",
    "print(worst_drawdowns_df)\n",
    "print(\"\\nAverage Drawdowns:\")\n",
    "print(average_drawdowns_df)\n",
    "print(\"\\nAverage Drawdown Durations:\")\n",
    "print(average_drawdown_durations_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_drawdowns_df.sort_values(by='Worst Drawdown', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_drawdowns_df.sort_values(by='Average Drawdown', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_drawdown_durations_df.sort_values(by='Average Drawdown Duration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pre-defined list of columns to plot\n",
    "columns_to_plot = ['bo fast/slow + long skew', 'bo fast/slow + comb skew', 'rsi_slow_tf', 'Skew_Long_252']\n",
    "# Plot histogram\n",
    "plt.figure(figsize=(12, 8))\n",
    "for col in columns_to_plot:\n",
    "    plt.hist(scaled_ret[col], bins=200, alpha=0.5, label=col)\n",
    "\n",
    "\n",
    "plt.axvline(x=0, color='r', linestyle='--')\n",
    "plt.xlabel('Daily PnL')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Daily PnL History')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ensure that 'Date' is the index and in datetime format if not already\n",
    "pnl_series = scaled_ret['bo fast/slow + long skew']\n",
    "pnl_series.index = pd.to_datetime(pnl_series.index)\n",
    "\n",
    "# Resample to monthly frequency and sum the PnL for each month\n",
    "monthly_pnl = pnl_series.resample('M').sum()\n",
    "\n",
    "# Create a DataFrame for monthly PnL\n",
    "monthly_pnl_df = monthly_pnl.to_frame(name='Skew Long and BO L Pullback').reset_index()\n",
    "monthly_pnl_df['Year'] = monthly_pnl_df['Date'].dt.year\n",
    "monthly_pnl_df['Month'] = monthly_pnl_df['Date'].dt.month_name().str[:3]\n",
    "\n",
    "# Pivot the DataFrame to get the desired format\n",
    "monthly_pnl_pivot = monthly_pnl_df.pivot(index='Year', columns='Month', values='Skew Long and BO L Pullback')\n",
    "monthly_pnl_pivot = monthly_pnl_pivot[['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']]\n",
    "\n",
    "# Calculate the total PnL for each year\n",
    "monthly_pnl_pivot['Total'] = monthly_pnl_pivot.sum(axis=1)\n",
    "\n",
    "# Calculate annual Sharpe using daily data\n",
    "# Group daily data by year for the sum of returns and volatility\n",
    "annual_returns = pnl_series.resample('Y').sum()  # Total return per year\n",
    "annual_volatility = pnl_series.resample('Y').std() * (252 ** 0.5)  # Annualized volatility\n",
    "\n",
    "# Calculate Sharpe ratio and handle cases with zero volatility\n",
    "annual_sharpe = (annual_returns / annual_volatility).replace([float('inf'), -float('inf')], pd.NA)\n",
    "\n",
    "# Merge annual Sharpe ratio into the pivot table\n",
    "monthly_pnl_pivot['Sharpe'] = annual_sharpe.values\n",
    "\n",
    "# Add the annual volatility and worst drawdown columns to the pivot table\n",
    "monthly_pnl_pivot['Annual Volatility'] = annual_volatility.values\n",
    "# Calculate the worst drawdown for each year\n",
    "annual_worst_drawdown = pnl_series.resample('Y').apply(lambda x: (x.cumsum() - x.cumsum().cummax()).min())\n",
    "\n",
    "# Merge annual worst drawdown into the pivot table\n",
    "monthly_pnl_pivot['Worst Drawdown'] = annual_worst_drawdown.values\n",
    "\n",
    "monthly_pnl_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ensure that 'Date' is the index and in datetime format if not already\n",
    "pnl_series = scaled_ret['bo slow + long skew']\n",
    "pnl_series.index = pd.to_datetime(pnl_series.index)\n",
    "\n",
    "# Resample to monthly frequency and sum the PnL for each month\n",
    "monthly_pnl = pnl_series.resample('M').sum()\n",
    "\n",
    "# Create a DataFrame for monthly PnL\n",
    "monthly_pnl_df = monthly_pnl.to_frame(name='Skew Long and BO L Pullback').reset_index()\n",
    "monthly_pnl_df['Year'] = monthly_pnl_df['Date'].dt.year\n",
    "monthly_pnl_df['Month'] = monthly_pnl_df['Date'].dt.month_name().str[:3]\n",
    "\n",
    "# Pivot the DataFrame to get the desired format\n",
    "monthly_pnl_pivot = monthly_pnl_df.pivot(index='Year', columns='Month', values='Skew Long and BO L Pullback')\n",
    "monthly_pnl_pivot = monthly_pnl_pivot[['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']]\n",
    "\n",
    "# Calculate the total PnL for each year\n",
    "monthly_pnl_pivot['Total'] = monthly_pnl_pivot.sum(axis=1)\n",
    "\n",
    "# Calculate annual Sharpe using daily data\n",
    "# Group daily data by year for the sum of returns and volatility\n",
    "annual_returns = pnl_series.resample('Y').sum()  # Total return per year\n",
    "annual_volatility = pnl_series.resample('Y').std() * (252 ** 0.5)  # Annualized volatility\n",
    "\n",
    "# Calculate Sharpe ratio and handle cases with zero volatility\n",
    "annual_sharpe = (annual_returns / annual_volatility).replace([float('inf'), -float('inf')], pd.NA)\n",
    "\n",
    "# Merge annual Sharpe ratio into the pivot table\n",
    "monthly_pnl_pivot['Sharpe'] = annual_sharpe.values\n",
    "monthly_pnl_pivot['Annual Volatility'] = annual_volatility.values\n",
    "annual_worst_drawdown = pnl_series.resample('Y').apply(lambda x: (x.cumsum() - x.cumsum().cummax()).min())\n",
    "\n",
    "# Merge annual worst drawdown into the pivot table\n",
    "monthly_pnl_pivot['Worst Drawdown'] = annual_worst_drawdown.values\n",
    "\n",
    "\n",
    "\n",
    "monthly_pnl_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ensure that 'Date' is the index and in datetime format if not already\n",
    "pnl_series = scaled_ret['bo slow + long skew']\n",
    "pnl_series.index = pd.to_datetime(pnl_series.index)\n",
    "\n",
    "# Resample to monthly frequency and sum the PnL for each month\n",
    "monthly_pnl = pnl_series.resample('M').sum()\n",
    "\n",
    "# Create a DataFrame for monthly PnL\n",
    "monthly_pnl_df = monthly_pnl.to_frame(name='bo slow + long skew').reset_index()\n",
    "monthly_pnl_df['Year'] = monthly_pnl_df['Date'].dt.year\n",
    "monthly_pnl_df['Month'] = monthly_pnl_df['Date'].dt.month_name().str[:3]\n",
    "\n",
    "# Pivot the DataFrame to get the desired format\n",
    "monthly_pnl_pivot = monthly_pnl_df.pivot(index='Year', columns='Month', values='bo slow + long skew')\n",
    "monthly_pnl_pivot = monthly_pnl_pivot[['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']]\n",
    "\n",
    "# Calculate the total PnL for each year\n",
    "monthly_pnl_pivot['Total'] = monthly_pnl_pivot.sum(axis=1)\n",
    "\n",
    "# Calculate annual Sharpe using daily data\n",
    "# Group daily data by year for the sum of returns and volatility\n",
    "annual_returns = pnl_series.resample('Y').sum()  # Total return per year\n",
    "annual_volatility = pnl_series.resample('Y').std() * (252 ** 0.5)  # Annualized volatility\n",
    "\n",
    "# Calculate Sharpe ratio and handle cases with zero volatility\n",
    "annual_sharpe = (annual_returns / annual_volatility).replace([float('inf'), -float('inf')], pd.NA)\n",
    "\n",
    "# Merge annual Sharpe ratio into the pivot table\n",
    "monthly_pnl_pivot['Sharpe'] = annual_sharpe.values\n",
    "\n",
    "monthly_pnl_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the rolling window size\n",
    "rolling_window = 125\n",
    "\n",
    "# Initialize a dictionary to store the rolling Sharpe ratios\n",
    "rolling_sharpe_ratios = {}\n",
    "\n",
    "scaled_ret_filt = scaled_ret[['bo slow + long skew', 'bo slow + comb skew', 'bo fast/slow + long skew', 'bo fast/slow + comb skew']]\n",
    "\n",
    "# Calculate the rolling Sharpe ratio for each strategy\n",
    "for col in scaled_ret_filt.columns:\n",
    "    rolling_mean = scaled_ret[col].rolling(window=rolling_window).mean()\n",
    "    rolling_std = scaled_ret[col].rolling(window=rolling_window).std()\n",
    "    annualized_mean = rolling_mean * 252\n",
    "    annualized_std = rolling_std * (252 ** 0.5)\n",
    "    rolling_sharpe = annualized_mean / annualized_std\n",
    "    rolling_sharpe_ratios[col] = rolling_sharpe\n",
    "\n",
    "# Plot the rolling Sharpe ratios\n",
    "plt.figure(figsize=(14, 10))\n",
    "for col, rolling_sharpe in rolling_sharpe_ratios.items():\n",
    "    plt.plot(rolling_sharpe, label=col)\n",
    "\n",
    "# Calculate the average of the rolling Sharpe ratios for each column\n",
    "average_rolling_sharpe = {col: rolling_sharpe.mean() for col, rolling_sharpe in rolling_sharpe_ratios.items()}\n",
    "\n",
    "# Plot horizontal lines for the average rolling Sharpe ratios\n",
    "for col, avg_sharpe in average_rolling_sharpe.items():\n",
    "    plt.axhline(y=avg_sharpe, linestyle='--', label=f'Avg {col}: {avg_sharpe:.2f}')\n",
    "\n",
    "\n",
    "plt.title('Rolling 125-Day Annualized Sharpe Ratios')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sharpe Ratio')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "currtr = trading_rules_dict['rsi_l_ml_h']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotdf = currtr.plot_equity(byassets=True, filter_ac=['eq-eu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnlt = currtr.perf_table(byac=True, metric='pnl')\n",
    "pnlt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tradeslist = currtr._load_tradeslist()\n",
    "tradeslist.to_clipboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perft = currtr.perf_table(byac=True, metric='sharpe')\n",
    "perft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca1 = currtr._get_btdf('CA1 Index')\n",
    "ca1.to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
